{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this excercise you are going to work on the Yelp database (that can be found here). The goal is to  predict if a comment  about a venue is bad or good. The database contains information such as venue ID, date of comment, rating and of course comments of the users. In order to use the information contained in the comments, you are gonna have to apply some preprocessing to the data.\n",
    "\n",
    "A \"bad\" venue is defined as a venue with a star rating of less or equal to two stars, the other venues being considered as \"good\". Note that this is for the sake of working with a classification problem. One could actually work with the number of stars and use regression algorithms (which could actually be a better idea).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you give as input variable the raw comments of the dataset, you model will be of extremely poor accuracy. Some transformations should be made on the comments to extract the necessary information and put it in a form appropriate for a machine learning model. \n",
    "\n",
    "You should first clean the raw text. The type of cleaning depends on which data you have and what you want to achieve : remove punctation, switch everything to lower case, remove words containing numbers, remove words not adding any information (like \"the\" or \"of\", usually called \"stopwords\"), taking only the root of the words (and therefore consider words with same root but different ending as similar),...  For example, you may want \"*I was disappointed, what a terrible quality of food!!!*\" to be tranformed into \"*disappointed terrible quality food*\".\n",
    "You can have a look at the most frequent words in every classes to have an idea of the quality of your cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hints : \n",
    " - You can use the \"ujson\" package to transform the data into a pandas dataframe (every row of the data file is json). \n",
    " - There are some build-in methods in pandas to transform a json into a dataframe.\n",
    " - You can retrieve a list of english stopwords in the CountVectorizer class of sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are satisfied with the cleaning, the text should be transformed into a vector readable by the machine learning model.\n",
    "There are different approaches to do so. A common and easy approach is the \"bag-of-words\", where sentences are split into a list of words. This approach does not take into account the order of the words but only their number of occurences. If we take our previous example, \"*disappointed terrible quality food*\", it would be first split as :\n",
    "\n",
    "[*\"disappointed\",\n",
    " \"terrible\",\n",
    " \"quality\",\n",
    " \"food\"*]\n",
    " \n",
    "This would be then transformed into a vector in the vector space of the vocabulary, where each dimension represents a word. The comment would be of the form\n",
    "[0,0,..,1, 0, 1,..,0]\n",
    "With value 1 for the dimension representing *disappointed*, *terrible*, *quality* or *food*. If one word appears 2 times, the coordinate of the vector in the corresponding dimension would be 2.\n",
    "\n",
    "Note that if the number of different words in your dataset is large, you may encounter performance issue. The dimension of the vector is indeed equal to the size of the vocabulary of your dataset, which can quickly be of the order of several hundred thousands. Reducing the size of the vocabulary is therefore almost always required. Again, there is not one single approach : you could decide to remove the least common words or to remove words appearing in almost all comments. This vectorization of text can be done with the CountVectorizer class of sicki-learn, which allows you also to decrease the size of the vocabulary. The way of giving a weight to a word is also not unique, you can use for example a tf-idf vectorizer (also implemented in sickit-learn) instead of the CountVectorizer class.\n",
    "\n",
    "Chosing the way of vectorizing the data is an iterative process. You will know exactly which vectorization to use when you see the end results of your model. It is the same as choosing a parameter of your model, you can have a first guess but you should try several values before knowing which one to use. To make your life easier, you can use a pipeline containing a step for preprocessing and a step for building the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When your text is vectorized, you can use it to train a machine learning model. The process is similar to what you have done before the linear regression. The difference is now you can use different models of classification such as logistic regression or random forest. All the main machine learning algorithm have an implementation in sickit-learn. They inherit from the same class and can therefore be used in a very similar way. To improve the result of your model you may consider different dimensions : type of model used, parameters of the model used and surely the way you preprocessed and vectorized your data (as explained in the previous sections)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different metrics can be used to evaluate if a classification model is good or not (f.ex. precision, recall, auc,..). You should easily achieve an auc for your testing set of about 0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hints :\n",
    " - You can use gridsearchCV to help you find the optimum parameters for your model (and also of your vectorization)\n",
    " - Watch out for overfitting issue, you can compare the precision of your model on the training set and on the testing set to detect if you are overfitting or not.\n",
    " - You should check if the number of occurence of \"bad\" review is similar to the number of \"good\" review. You should consider resampling the data if it's not the case to have a more balanced training dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
